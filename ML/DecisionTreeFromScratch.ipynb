{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSLCu11wfahn82IQ+YpyFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maverick2903/SEM6_Programs/blob/main/ML/DecisionTreeFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install treenode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvn2nq_4oI4e",
        "outputId": "c2092b85-fada-4e5e-e5dc-12343044e182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting treenode\n",
            "  Downloading treenode-0.1.5.zip (6.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: treenode\n",
            "  Building wheel for treenode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for treenode: filename=treenode-0.1.5-py3-none-any.whl size=3865 sha256=16f4bb78b6586d635203ae32b3ffcaa050d7902ec938b5d4042ec9f2e28a4351\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/86/01/de69a2f0e07598a056c0c043f1a7d47b5ed5ef7f6ad1bb9bf9\n",
            "Successfully built treenode\n",
            "Installing collected packages: treenode\n",
            "Successfully installed treenode-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from treenode import TreeNode\n",
        "\n",
        "\n",
        "class DecisionTree():\n",
        "    \"\"\"\n",
        "    Decision Tree Classifier\n",
        "    Training: Use \"train\" function with train set features and labels\n",
        "    Predicting: Use \"predict\" function with test set features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=4, min_samples_leaf=1,\n",
        "                 min_information_gain=0.0, numb_of_features_splitting=None,\n",
        "                 amount_of_say=None) -> None:\n",
        "        \"\"\"\n",
        "        Setting the class with hyperparameters\n",
        "        max_depth: (int) -> max depth of the tree\n",
        "        min_samples_leaf: (int) -> min # of samples required to be in a leaf to make the splitting possible\n",
        "        min_information_gain: (float) -> min information gain required to make the splitting possible\n",
        "        num_of_features_splitting: (str) ->  when splitting if sqrt then sqrt(# of features) features considered,\n",
        "                                                            if log then log(# of features) features considered\n",
        "                                                            else all features are considered\n",
        "        amount_of_say: (float) -> used for Adaboost algorithm\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.min_information_gain = min_information_gain\n",
        "        self.numb_of_features_splitting = numb_of_features_splitting\n",
        "        self.amount_of_say = amount_of_say\n",
        "\n",
        "    def _entropy(self, class_probabilities: list) -> float:\n",
        "        return sum([-p * np.log2(p) for p in class_probabilities if p>0])\n",
        "\n",
        "    def _class_probabilities(self, labels: list) -> list:\n",
        "        total_count = len(labels)\n",
        "        return [label_count / total_count for label_count in Counter(labels).values()]\n",
        "\n",
        "    def _data_entropy(self, labels: list) -> float:\n",
        "        return self._entropy(self._class_probabilities(labels))\n",
        "\n",
        "    def _partition_entropy(self, subsets: list) -> float:\n",
        "        \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1])\"\"\"\n",
        "        total_count = sum([len(subset) for subset in subsets])\n",
        "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
        "\n",
        "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
        "\n",
        "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
        "        group1 = data[mask_below_threshold]\n",
        "        group2 = data[~mask_below_threshold]\n",
        "\n",
        "        return group1, group2\n",
        "\n",
        "    def _select_features_to_use(self, data: np.array) -> list:\n",
        "        \"\"\"\n",
        "        Randomly selects the features to use while splitting w.r.t. hyperparameter numb_of_features_splitting\n",
        "        \"\"\"\n",
        "        feature_idx = list(range(data.shape[1]-1))\n",
        "\n",
        "        if self.numb_of_features_splitting == \"sqrt\":\n",
        "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))))\n",
        "        elif self.numb_of_features_splitting == \"log\":\n",
        "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))))\n",
        "        else:\n",
        "            feature_idx_to_use = feature_idx\n",
        "\n",
        "        return feature_idx_to_use\n",
        "\n",
        "    def _find_best_split(self, data: np.array) -> tuple:\n",
        "        \"\"\"\n",
        "        Finds the best split (with the lowest entropy) given data\n",
        "        Returns 2 splitted groups and split information\n",
        "        \"\"\"\n",
        "        min_part_entropy = 1e9\n",
        "        feature_idx_to_use =  self._select_features_to_use(data)\n",
        "\n",
        "        for idx in feature_idx_to_use:\n",
        "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
        "            for feature_val in feature_vals:\n",
        "                g1, g2, = self._split(data, idx, feature_val)\n",
        "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
        "                if part_entropy < min_part_entropy:\n",
        "                    min_part_entropy = part_entropy\n",
        "                    min_entropy_feature_idx = idx\n",
        "                    min_entropy_feature_val = feature_val\n",
        "                    g1_min, g2_min = g1, g2\n",
        "\n",
        "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
        "\n",
        "    def _find_label_probs(self, data: np.array) -> np.array:\n",
        "\n",
        "        labels_as_integers = data[:,-1].astype(int)\n",
        "        # Calculate the total number of labels\n",
        "        total_labels = len(labels_as_integers)\n",
        "        # Calculate the ratios (probabilities) for each label\n",
        "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
        "\n",
        "        # Populate the label_probabilities array based on the specific labels\n",
        "        for i, label in enumerate(self.labels_in_train):\n",
        "            label_index = np.where(labels_as_integers == i)[0]\n",
        "            if len(label_index) > 0:\n",
        "                label_probabilities[i] = len(label_index) / total_labels\n",
        "\n",
        "        return label_probabilities\n",
        "\n",
        "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
        "        \"\"\"\n",
        "        Recursive, depth first tree creation algorithm\n",
        "        \"\"\"\n",
        "\n",
        "        # Check if the max depth has been reached (stopping criteria)\n",
        "        if current_depth > self.max_depth:\n",
        "            return None\n",
        "\n",
        "        # Find best split\n",
        "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
        "\n",
        "        # Find label probs for the node\n",
        "        label_probabilities = self._find_label_probs(data)\n",
        "\n",
        "        # Calculate information gain\n",
        "        node_entropy = self._entropy(label_probabilities)\n",
        "        information_gain = node_entropy - split_entropy\n",
        "\n",
        "        # Create node\n",
        "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
        "\n",
        "        # Check if the min_samples_leaf has been satisfied (stopping criteria)\n",
        "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
        "            return node\n",
        "        # Check if the min_information_gain has been satisfied (stopping criteria)\n",
        "        elif information_gain < self.min_information_gain:\n",
        "            return node\n",
        "\n",
        "        current_depth += 1\n",
        "        node.left = self._create_tree(split_1_data, current_depth)\n",
        "        node.right = self._create_tree(split_2_data, current_depth)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
        "        \"\"\"Returns prediction for 1 dim array\"\"\"\n",
        "        node = self.tree\n",
        "\n",
        "        # Finds the leaf which X belongs\n",
        "        while node:\n",
        "            pred_probs = node.prediction_probs\n",
        "            if X[node.feature_idx] < node.feature_val:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "\n",
        "        return pred_probs\n",
        "\n",
        "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
        "        \"\"\"\n",
        "        Trains the model with given X and Y datasets\n",
        "        \"\"\"\n",
        "\n",
        "        # Concat features and labels\n",
        "        self.labels_in_train = np.unique(Y_train)\n",
        "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
        "\n",
        "        # Start creating the tree\n",
        "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
        "\n",
        "        # Calculate feature importance\n",
        "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
        "        self._calculate_feature_importance(self.tree)\n",
        "        # Normalize the feature importance values\n",
        "        self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()}\n",
        "\n",
        "    def predict_proba(self, X_set: np.array) -> np.array:\n",
        "        \"\"\"Returns the predicted probs for a given data set\"\"\"\n",
        "\n",
        "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
        "\n",
        "        return pred_probs\n",
        "\n",
        "    def predict(self, X_set: np.array) -> np.array:\n",
        "        \"\"\"Returns the predicted labels for a given data set\"\"\"\n",
        "\n",
        "        pred_probs = self.predict_proba(X_set)\n",
        "        preds = np.argmax(pred_probs, axis=1)\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
        "        if node != None:\n",
        "            self._print_recursive(node.left, level + 1)\n",
        "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
        "            self._print_recursive(node.right, level + 1)\n",
        "\n",
        "    def print_tree(self) -> None:\n",
        "        self._print_recursive(node=self.tree)\n",
        "\n",
        "    def _calculate_feature_importance(self, node):\n",
        "        \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\"\n",
        "        if node != None:\n",
        "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
        "            self._calculate_feature_importance(node.left)\n",
        "            self._calculate_feature_importance(node.right)"
      ],
      "metadata": {
        "id": "u9jvVysMoHFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "print('\\n\\nClassification Tree')\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state = 42)\n",
        "print(X_train.shape)\n",
        "dt = DecisionTree(max_depth=4,min_samples_leaf=1)\n",
        "dt.train(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "tpSEXZ7Aouee",
        "outputId": "468d6831-b833-4b89-8ba5-591956d578be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Classification Tree\n",
            "(112, 4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "TreeNode.__init__() takes 1 positional argument but 6 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a03d90ccbf51>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-c83016c3e694>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Start creating the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Calculate feature importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c83016c3e694>\u001b[0m in \u001b[0;36m_create_tree\u001b[0;34m(self, data, current_depth)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Create node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_feature_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Check if the min_samples_leaf has been satisfied (stopping criteria)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TreeNode.__init__() takes 1 positional argument but 6 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CART(object):\n",
        "    def __init__(self, tree = 'cls', criterion = 'gini', prune = 'depth', max_depth = 4, min_criterion = 0.05):\n",
        "        self.feature = None\n",
        "        self.label = None\n",
        "        self.n_samples = None\n",
        "        self.gain = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.threshold = None\n",
        "        self.depth = 0\n",
        "\n",
        "        self.root = None\n",
        "        self.criterion = criterion\n",
        "        self.prune = prune\n",
        "        self.max_depth = max_depth\n",
        "        self.min_criterion = min_criterion\n",
        "        self.tree = tree\n",
        "\n",
        "    def fit(self, features, target):\n",
        "        self.root = CART()\n",
        "        if(self.tree == 'cls'):\n",
        "            self.root._grow_tree(features, target, self.criterion)\n",
        "        else:\n",
        "            self.root._grow_tree(features, target, 'mse')\n",
        "        self.root._prune(self.prune, self.max_depth, self.min_criterion, self.root.n_samples)\n",
        "\n",
        "    def predict(self, features):\n",
        "        return np.array([self.root._predict(f) for f in features])\n",
        "\n",
        "    def print_tree(self):\n",
        "        self.root._show_tree(0, ' ')\n",
        "\n",
        "    def _grow_tree(self, features, target, criterion = 'gini'):\n",
        "        self.n_samples = features.shape[0]\n",
        "\n",
        "        if len(np.unique(target)) == 1:\n",
        "            self.label = target[0]\n",
        "            return\n",
        "\n",
        "        best_gain = 0.0\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        if criterion in {'gini', 'entropy'}:\n",
        "            self.label = max([(c, len(target[target == c])) for c in np.unique(target)], key = lambda x : x[1])[0]\n",
        "        else:\n",
        "            self.label = np.mean(target)\n",
        "\n",
        "        impurity_node = self._calc_impurity(criterion, target)\n",
        "\n",
        "        for col in range(features.shape[1]):\n",
        "            feature_level = np.unique(features[:,col])\n",
        "            thresholds = (feature_level[:-1] + feature_level[1:]) / 2.0\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                target_l = target[features[:,col] <= threshold]\n",
        "                impurity_l = self._calc_impurity(criterion, target_l)\n",
        "                n_l = float(target_l.shape[0]) / self.n_samples\n",
        "\n",
        "                target_r = target[features[:,col] > threshold]\n",
        "                impurity_r = self._calc_impurity(criterion, target_r)\n",
        "                n_r = float(target_r.shape[0]) / self.n_samples\n",
        "\n",
        "                impurity_gain = impurity_node - (n_l * impurity_l + n_r * impurity_r)\n",
        "                if impurity_gain > best_gain:\n",
        "                    best_gain = impurity_gain\n",
        "                    best_feature = col\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        self.feature = best_feature\n",
        "        self.gain = best_gain\n",
        "        self.threshold = best_threshold\n",
        "        self._split_tree(features, target, criterion)\n",
        "\n",
        "    def _split_tree(self, features, target, criterion):\n",
        "        features_l = features[features[:, self.feature] <= self.threshold]\n",
        "        target_l = target[features[:, self.feature] <= self.threshold]\n",
        "        self.left = CART()\n",
        "        self.left.depth = self.depth + 1\n",
        "        self.left._grow_tree(features_l, target_l, criterion)\n",
        "\n",
        "        features_r = features[features[:, self.feature] > self.threshold]\n",
        "        target_r = target[features[:, self.feature] > self.threshold]\n",
        "        self.right = CART()\n",
        "        self.right.depth = self.depth + 1\n",
        "        self.right._grow_tree(features_r, target_r, criterion)\n",
        "\n",
        "    def _calc_impurity(self, criterion, target):\n",
        "        if criterion == 'gini':\n",
        "            return 1.0 - sum([(float(len(target[target == c])) / float(target.shape[0])) ** 2.0 for c in np.unique(target)])\n",
        "        elif criterion == 'mse':\n",
        "            return np.mean((target - np.mean(target)) ** 2.0)\n",
        "        else:\n",
        "            entropy = 0.0\n",
        "            for c in np.unique(target):\n",
        "                p = float(len(target[target == c])) / target.shape[0]\n",
        "                if p > 0.0:\n",
        "                    entropy -= p * np.log2(p)\n",
        "            return entropy\n",
        "\n",
        "    def _prune(self, method, max_depth, min_criterion, n_samples):\n",
        "        if self.feature is None:\n",
        "            return\n",
        "\n",
        "        self.left._prune(method, max_depth, min_criterion, n_samples)\n",
        "        self.right._prune(method, max_depth, min_criterion, n_samples)\n",
        "\n",
        "        pruning = False\n",
        "\n",
        "        if method == 'impurity' and self.left.feature is None and self.right.feature is None:\n",
        "            if (self.gain * float(self.n_samples) / n_samples) < min_criterion:\n",
        "                pruning = True\n",
        "        elif method == 'depth' and self.depth >= max_depth:\n",
        "            pruning = True\n",
        "\n",
        "        if pruning is True:\n",
        "            self.left = None\n",
        "            self.right = None\n",
        "            self.feature = None\n",
        "\n",
        "    def _predict(self, d):\n",
        "        if self.feature != None:\n",
        "            if d[self.feature] <= self.threshold:\n",
        "                return self.left._predict(d)\n",
        "            else:\n",
        "                return self.right._predict(d)\n",
        "        else:\n",
        "            return self.label\n",
        "\n",
        "    def _show_tree(self, depth, cond):\n",
        "        base = '    ' * depth + cond\n",
        "        if self.feature != None:\n",
        "            print(base + 'if X[' + str(self.feature) + '] <= ' + str(self.threshold))\n",
        "            self.left._show_tree(depth+1, 'then ')\n",
        "            self.right._show_tree(depth+1, 'else ')\n",
        "        else:\n",
        "            print(base + '{value: ' + str(self.label) + ', samples: ' + str(self.n_samples) + '}')\n"
      ],
      "metadata": {
        "id": "oOHVC5Xnpjqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree as sktree\n",
        "\n",
        "def classification_example():\n",
        "    print('\\n\\nClassification Tree')\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
        "\n",
        "    cls = CART(tree = 'cls', criterion = 'entropy', prune = 'depth', max_depth = 3)\n",
        "    cls.fit(X_train, y_train)\n",
        "    cls.print_tree()\n",
        "\n",
        "    pred = cls.predict(X_test)\n",
        "    print(\"This Classification Tree Prediction Accuracy:    {}\".format(sum(pred == y_test) / len(pred)))\n",
        "\n",
        "    clf = sktree.DecisionTreeClassifier(criterion = 'entropy')\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    sk_pred = clf.predict(X_test)\n",
        "\n",
        "    print(\"Sklearn Library Tree Prediction Accuracy:        {}\".format(sum(sk_pred == y_test) / len(pred)))\n",
        "\n",
        "def regression_example():\n",
        "    print('\\n\\nRegression Tree')\n",
        "    rng = np.random.RandomState(1)\n",
        "    X = np.sort(5 * rng.rand(80, 1), axis = 0)\n",
        "    y = np.sin(X).ravel()\n",
        "    y[::5] += 3 * (0.5 - rng.rand(16))\n",
        "\n",
        "    # Fit regression model\n",
        "    reg = CART(tree = 'reg', criterion = 'mse', prune = 'depth', max_depth = 2)\n",
        "    reg.fit(X, y)\n",
        "    reg.print_tree()\n",
        "\n",
        "    pred = reg.predict(np.sort(5 * rng.rand(1, 1), axis = 0))\n",
        "    print('This Regression Tree Prediction:            {}'.format(pred))\n",
        "\n",
        "    sk_reg = sktree.DecisionTreeRegressor(max_depth = 3)\n",
        "    sk_reg.fit(X, y)\n",
        "    sk_pred = sk_reg.predict(np.sort(5 * rng.rand(1, 1), axis = 0))\n",
        "    print('Sklearn Library Regression Tree Prediction: {}'.format(sk_pred))\n",
        "\n",
        "classification_example()\n",
        "regression_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBqLL-r8qw3p",
        "outputId": "b29178bc-e467-4adf-8857-3ac8735afb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Classification Tree\n",
            " if X[2] <= 2.45\n",
            "    then {value: 0, samples: 35}\n",
            "    else if X[2] <= 4.75\n",
            "        then if X[3] <= 1.65\n",
            "            then {value: 1, samples: 34}\n",
            "            else {value: 2, samples: 1}\n",
            "        else if X[2] <= 5.15\n",
            "            then {value: 2, samples: 16}\n",
            "            else {value: 2, samples: 26}\n",
            "This Classification Tree Prediction Accuracy:    0.9736842105263158\n",
            "Sklearn Library Tree Prediction Accuracy:        0.9736842105263158\n",
            "\n",
            "\n",
            "Regression Tree\n",
            " if X[0] <= 3.132750455307683\n",
            "    then if X[0] <= 0.5139010885136697\n",
            "        then {value: 0.052360677956274054, samples: 11}\n",
            "        else {value: 0.7138256817142807, samples: 40}\n",
            "    else if X[0] <= 3.8502285789691104\n",
            "        then {value: -0.45190263977283723, samples: 14}\n",
            "        else {value: -0.8686425569863078, samples: 15}\n",
            "This Regression Tree Prediction:            [-0.86864256]\n",
            "Sklearn Library Regression Tree Prediction: [0.52672009]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "def entropy(y):\n",
        "    hist = np.bincount(y)\n",
        "    ps = hist / len(y)\n",
        "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
        "class Node:\n",
        "\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "class DecisionTree:\n",
        "\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        #stopping criteria\n",
        "        if (depth >= self.max_depth\n",
        "                or n_labels == 1\n",
        "                or n_samples < self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
        "\n",
        "        #greedily select the best split according to information gain\n",
        "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
        "\n",
        "        #grow the children that result from the split\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feat, best_thresh, left, right)\n",
        "\n",
        "    def _best_criteria(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_thresh = threshold\n",
        "\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _information_gain(self, y, X_column, split_thresh):\n",
        "        #parent loss\n",
        "        parent_entropy = entropy(y)\n",
        "\n",
        "        #generate split\n",
        "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        #compute the weighted avg. of the loss for the children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
        "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
        "\n",
        "        #information gain is difference in loss before vs. after split\n",
        "        ig = parent_entropy - child_entropy\n",
        "        return ig\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "def accuracy(y_true, y_pred):\n",
        "    accuracy = np.sum(y_true == y_pred)/len(y_true)\n",
        "    return accuracy\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=123)\n",
        "\n",
        "clf = DecisionTree(max_depth = 10)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred1 = clf.predict(X_train)\n",
        "acc1 = accuracy(y_train, y_pred1)\n",
        "print(\"Training Accuracy: \", acc1)\n",
        "\n",
        "y_pred2 = clf.predict(X_test)\n",
        "acc2 = accuracy(y_test, y_pred2)\n",
        "print(\"Testing Accuracy: \", acc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNjz8Bazu3QY",
        "outputId": "9977dde0-729f-4215-d427-8efa97822b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  1.0\n",
            "Testing Accuracy:  0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value # for leaf nodes\n",
        "\n",
        "class CARTDecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        best_gini = float('inf')\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature] <= threshold\n",
        "                right_indices = X[:, feature] > threshold\n",
        "                gini = self._gini_impurity(y[left_indices]) + self._gini_impurity(y[right_indices])\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        classes = np.unique(y)\n",
        "        n_samples = len(y)\n",
        "        gini = 1\n",
        "        for c in classes:\n",
        "            p = np.sum(y == c) / n_samples\n",
        "            gini -= p ** 2\n",
        "        return gini\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(np.unique(y))\n",
        "\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or n_classes == 1 or n_samples < 2:\n",
        "            print(\"Leaf Node: \",np.argmax(np.bincount(y)))\n",
        "            return Node(value=np.argmax(np.bincount(y)))\n",
        "\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            print(\"Leaf Node: \",np.argmax(np.bincount(y)))\n",
        "            return Node(value=np.argmax(np.bincount(y)))\n",
        "\n",
        "        left_indices = X[:, best_feature] <= best_threshold\n",
        "        right_indices = X[:, best_feature] > best_threshold\n",
        "        print(\"Splitting at node: feature {}, threshold {}\".format(best_feature,best_threshold))\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._predict_one(x, node.left)\n",
        "        else:\n",
        "            return self._predict_one(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict_one(x, self.root) for x in X]\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the CART decision tree\n",
        "tree = CARTDecisionTree(max_depth=5)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Test the trained model\n",
        "predictions = tree.predict(X_test)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md699nUUwfkP",
        "outputId": "470cf20f-efe0-42c2-c3c2-4ef77b0e88fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting at node: feature 22, threshold 117.2\n",
            "Splitting at node: feature 27, threshold 0.175\n",
            "Splitting at node: feature 7, threshold 0.08534\n",
            "Splitting at node: feature 10, threshold 0.8811\n",
            "Splitting at node: feature 0, threshold 7.691\n",
            "Leaf Node:  1\n",
            "Leaf Node:  1\n",
            "Leaf Node:  0\n",
            "Leaf Node:  0\n",
            "Leaf Node:  0\n",
            "Splitting at node: feature 8, threshold 0.122\n",
            "Leaf Node:  1\n",
            "Splitting at node: feature 24, threshold 0.08774\n",
            "Leaf Node:  1\n",
            "Leaf Node:  0\n",
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    }
  ]
}